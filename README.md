# ゼロから作る Deep Learning

[<img src="https://raw.githubusercontent.com/oreilly-japan/deep-learning-from-scratch/images/deep-learning-from-scratch.png" width="200px">](https://www.oreilly.co.jp/books/9784873117584/)

## 2. パーセプトロン（Perceptron）

AND, OR, NAND, XOR

## 3. ニューラルネットワーク（NN : Neural Network）

### 3.1 パーセプトロンからニューラルネットワークへ

入力信号の総和を出力信号に変換する関数は、活性化関数（Activation Function）と呼ばれる。
活性化関数は入力信号の総和がどのように活性化するか（どのように発火するか）を決定する役割です。

活性化関数がパーセプトロンからニューラルネットワークへ進むための架け橋になる。

### 3.2 活性化関数

[ステップ関数のグラフ](ch03/step_function.py)

[シグモイド関数のグラフ](ch03/sigmoid.py)

[シグモイド関数とステップ関数の比較](ch03/sig_step_compare.py)

ステップ関数とシグモイド関数はともに非線形関数である。
ニューラルネットワークの活性化関数に非線形関数を用いる必要がある。
なぜなら、線形関数を用いるとニューラルネットワークで層を深くする意味がなくなってしまう。

シグモイド関数はニューラルネットワークの歴史上、古くから利用されてきたが、
最近はReLU（Rectified Linear Unit）という関数が主に利用されている。

[ReLU関数](ch03/relu.py)

### 3.3 多次元配列の計算

Numpyの多次元配列を使った計算をマスターすれば、ニューラルネットワークの実装を効率的に進める。

### 3.4 3層ニューラルネットワークの実装

### 3.5 出力層の設計

ニューラルネットワークは、分類問題と回帰問題の両方に用いることができる。
分類問題か回帰問題かで、出力層の活性化関数を変更する必要がある。
一般的に回帰問題は恒等関数、分類問題はソフトマックス関数を使用する。

ソフトマックス関数の出力は、0から1.0の間の実数になる。
また、ソフトマックス関数の出力の総和は1になるため、
ソフトマックス関数の出力を「確率」と解釈することができる。

### 3.6 手書き数字認識

MNISTデータセットは手書き数字の画像セットで、0から9までの数字画像から構成される。
訓練画像が60,000枚、テスト画像が10,000枚用意されている。

[MNIST訓練画像の1枚の表示](ch03/mnist_show.py)

[3層ニューラルネットワークの推論処理](ch03/neuralnet_mnist.py)

[100枚の画像を1バッチとして纏めて推論処理](ch03/neuralnet_mnist_batch.py)
